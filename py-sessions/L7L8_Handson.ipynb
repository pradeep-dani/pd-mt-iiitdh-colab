{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pradeep-dani/pd-mt-iiitdh-colab/blob/main/L7L8_Handson.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**🎯 ACTIVITY 1: STUDENT GRADE TRACKER**  \n",
        "**Topic**: Creating and Exploring DataFrames  \n",
        "**Duration**: 10 minutes  \n",
        "**Difficulty**: Beginner  \n",
        "## **Problem Statement**  \n",
        "You are a teacher managing student grades. Create a DataFrame to store student information and explore the data to understand your class performance.  \n",
        "Instructions  \n",
        "1.\tCreate a DataFrame with student data (provided below)\n",
        "2.\tDisplay basic information about the dataset\n",
        "3.\tFind the top 3 performing students\n",
        "4.\tCalculate basic statistics  \n",
        "Sample Data  \n",
        "student_data = {\n",
        "    'student_id': [101, 102, 103, 104, 105, 106, 107, 108],\n",
        "    'name': ['Aarav', 'Rohan', 'Ananya', 'Abdullah', 'Kavya', 'Siddharth', 'Neha', 'Vikram'],\n",
        "    'subject': ['Math', 'Physics', 'Chemistry', 'Math', 'Physics', 'Chemistry', 'Math', 'Physics'],\n",
        "    'score': [85, 78, 92, 88, 76, 95, 91, 82],\n",
        "    'attendance': [95, 88, 97, 92, 85, 98, 94, 89]\n",
        "}  \n",
        "Your Tasks  \n",
        "**Task 1:** Create DataFrame and display first 3 rows  \n",
        "**Task 2:** Show basic information about the dataset  \n",
        "**Task 3:** Find students with scores above 90  \n",
        "**Task 4:** Calculate average score and attendance  \n"
      ],
      "metadata": {
        "id": "iVUx9Fd62gG5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pzT7s1nVaFKh"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Given data\n",
        "student_data = {\n",
        "    'student_id': [101, 102, 103, 104, 105, 106, 107, 108],\n",
        "    'name': ['Aarav', 'Rohan', 'Ananya', 'Abdullah', 'Kavya', 'Siddharth', 'Neha', 'Vikram'],\n",
        "    'subject': ['Math', 'Physics', 'Chemistry', 'Math', 'Physics', 'Chemistry', 'Math', 'Physics'],\n",
        "    'score': [85, 78, 92, 88, 76, 95, 91, 82],\n",
        "    'attendance': [95, 88, 97, 92, 85, 98, 94, 89]\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hTdPr2u46tkH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 1: Create DataFrame and display first 3 rows\n",
        "students_df = pd.DataFrame(student_data)\n",
        "print(\"First 3 rows:\")\n",
        "print(students_df.head(3))\n",
        "print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q-jHJRpaaI52",
        "outputId": "c371e8ac-4d82-4e7a-d288-4b9cce173b5f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First 3 rows:\n",
            "   student_id    name    subject  score  attendance\n",
            "0         101   Aarav       Math     85          95\n",
            "1         102   Rohan    Physics     78          88\n",
            "2         103  Ananya  Chemistry     92          97\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Bpz-LFH6a3Q7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 2: Show basic information about the dataset\n",
        "print(\"Dataset Info:\")\n",
        "print(f\"Shape: {students_df.shape}\")\n",
        "print(f\"Columns: {list(students_df.columns)}\")\n",
        "print(\"\\nData Types:\")\n",
        "print(students_df.dtypes)\n",
        "print(\"\\nBasic Statistics:\")\n",
        "print(students_df.describe())\n",
        "print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dQwd4r_raMPh",
        "outputId": "99ba5fcb-99d9-46a5-cf39-ec9eff6b5889"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset Info:\n",
            "Shape: (8, 5)\n",
            "Columns: ['student_id', 'name', 'subject', 'score', 'attendance']\n",
            "\n",
            "Data Types:\n",
            "student_id     int64\n",
            "name          object\n",
            "subject       object\n",
            "score          int64\n",
            "attendance     int64\n",
            "dtype: object\n",
            "\n",
            "Basic Statistics:\n",
            "       student_id      score  attendance\n",
            "count     8.00000   8.000000    8.000000\n",
            "mean    104.50000  85.875000   92.250000\n",
            "std       2.44949   6.833479    4.590363\n",
            "min     101.00000  76.000000   85.000000\n",
            "25%     102.75000  81.000000   88.750000\n",
            "50%     104.50000  86.500000   93.000000\n",
            "75%     106.25000  91.250000   95.500000\n",
            "max     108.00000  95.000000   98.000000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "i_fVZnX8a5t_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-x8u8ojLa5w9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 3: Find students with scores above 90\n",
        "print(\"Students with scores above 90:\")\n",
        "high_performers = students_df[students_df['score'] > 90]\n",
        "print(high_performers[['name', 'subject', 'score']])\n",
        "print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tNStUtbEaPpk",
        "outputId": "63fd25a6-5aa2-4106-a4b3-6ed3d405f4c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Students with scores above 90:\n",
            "        name    subject  score\n",
            "2     Ananya  Chemistry     92\n",
            "5  Siddharth  Chemistry     95\n",
            "6       Neha       Math     91\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AF5s74Wja9DN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 4: Calculate average score and attendance\n",
        "avg_score = students_df['score'].mean()\n",
        "avg_attendance = students_df['attendance'].mean()\n",
        "print(f\"Average Score: {avg_score:.2f}\")\n",
        "print(f\"Average Attendance: {avg_attendance:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9lHySPXMaRjz",
        "outputId": "f35edb2d-b44f-49ab-e5dc-3ac9e8bfe6cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Score: 85.88\n",
            "Average Attendance: 92.25%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hc-WHw1Aa-4j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KG6XyhQ3a-_1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Bonus: Top 3 students by score\n",
        "print(\"\\nTop 3 Students by Score:\")\n",
        "top_students = students_df.nlargest(3, 'score')\n",
        "print(top_students[['name', 'subject', 'score','attendance']])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U5Bw3nvjaT0v",
        "outputId": "cd5c3eb7-8c72-4dc4-cba4-704826c3a705"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Top 3 Students by Score:\n",
            "        name    subject  score  attendance\n",
            "5  Siddharth  Chemistry     95          98\n",
            "2     Ananya  Chemistry     92          97\n",
            "6       Neha       Math     91          94\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**🎯 ACTIVITY 2: LIBRARY BOOK MANAGEMENT**  \n",
        "**Topic**: Data Selection and Filtering  \n",
        "**Duration**: 12 minutes  \n",
        "**Difficulty**: Beginner-Intermediate  \n",
        "## **Problem Statement**  \n",
        "You're managing a library system. Use pandas to filter and select books based on different criteria to help librarians and students find what they need.  \n",
        "**Sample Data**  \n",
        "library_data = {\n",
        "    'book_id': ['B001', 'B002', 'B003', 'B004', 'B005', 'B006', 'B007', 'B008'],\n",
        "    'title': ['Python Basics', 'Data Science Guide', 'Web Development', 'Machine Learning',\n",
        "              'Database Design', 'Statistics 101', 'AI Fundamentals', 'Programming Logic'],\n",
        "    'author': ['R. K. Narayan', 'Arundhati Roy', 'Chetan Bhagat', 'Ruskin Bond', 'Anita Desai', 'Vikram Seth', 'Jhumpa Lahiri', 'Amish Tripathi'],\n",
        "    'category': ['Programming', 'Data Science', 'Web Dev', 'ML', 'Database', 'Math', 'AI', 'Programming'],\n",
        "    'year': [2020, 2021, 2019, 2022, 2018, 2020, 2023, 2019],\n",
        "    'pages': [250, 400, 300, 500, 200, 350, 450, 180],\n",
        "    'available': [True, False, True, True, False, True, True, False]\n",
        "}  \n",
        "Your Tasks  \n",
        "**Task 1:** Select books published after 2020  \n",
        "**Task 2:** Find available Programming books  \n",
        "**Task 3:** Get books with more than 300 pages that are available  \n",
        "**Task 4:** Select specific columns for books by author 'Arundhati Roy' or 'Ruskin Bond'  \n"
      ],
      "metadata": {
        "id": "1I9eg4v27t4i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Given data\n",
        "library_data = {\n",
        "    'book_id': ['B001', 'B002', 'B003', 'B004', 'B005', 'B006', 'B007', 'B008'],\n",
        "    'title': ['Python Basics', 'Data Science Guide', 'Web Development', 'Machine Learning',\n",
        "              'Database Design', 'Statistics 101', 'AI Fundamentals', 'Programming Logic'],\n",
        "    'author': ['R. K. Narayan', 'Arundhati Roy', 'Chetan Bhagat', 'Ruskin Bond', 'Anita Desai', 'Vikram Seth', 'Jhumpa Lahiri', 'Amish Tripathi'],\n",
        "    'category': ['Programming', 'Data Science', 'Web Dev', 'ML', 'Database', 'Math', 'AI', 'Programming'],\n",
        "    'year': [2020, 2021, 2019, 2022, 2018, 2020, 2023, 2019],\n",
        "    'pages': [250, 400, 300, 500, 200, 350, 450, 180],\n",
        "    'available': [True, False, True, True, False, True, True, False]\n",
        "}"
      ],
      "metadata": {
        "id": "vGngKnCtaWA2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create DataFrame\n",
        "library_df = pd.DataFrame(library_data)"
      ],
      "metadata": {
        "id": "h5SVsqKZ9CfF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 1: Select books published after 2020\n",
        "print(\"Books published after 2020:\")\n",
        "recent_books = library_df[library_df['year'] > 2020]\n",
        "print(recent_books[['title', 'author', 'year']])\n",
        "print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x79sCPao9Ejv",
        "outputId": "6171d899-1eec-4437-8ac9-4743f4bcaab4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Books published after 2020:\n",
            "                title         author  year\n",
            "1  Data Science Guide  Arundhati Roy  2021\n",
            "3    Machine Learning    Ruskin Bond  2022\n",
            "6     AI Fundamentals  Jhumpa Lahiri  2023\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 2: Find available Programming books\n",
        "print(\"Available Programming books:\")\n",
        "available_programming = library_df[(library_df['category'] == 'Programming') & (library_df['available'] == True)]\n",
        "print(available_programming[['title', 'author', 'available']])\n",
        "print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rihjIq8A9GiE",
        "outputId": "93507bff-96c3-4383-bee1-7643a802d193"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Available Programming books:\n",
            "           title         author  available\n",
            "0  Python Basics  R. K. Narayan       True\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 3: Get books with more than 300 pages that are available\n",
        "print(\"Available books with more than 300 pages:\")\n",
        "big_available_books = library_df[(library_df['pages'] > 300) & (library_df['available'] == True)]\n",
        "print(big_available_books[['title', 'pages', 'available']])\n",
        "print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f6p1ad4b9InC",
        "outputId": "bb77846c-59ce-41d6-be84-cdce60bf0337"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Available books with more than 300 pages:\n",
            "              title  pages  available\n",
            "3  Machine Learning    500       True\n",
            "5    Statistics 101    350       True\n",
            "6   AI Fundamentals    450       True\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 4: Select specific columns for books by author 'Arundhati Roy' or 'Ruskin Bond'\n",
        "print(\"Books by Arundhati Roy or Ruskin Bond:\")\n",
        "specific_authors = library_df[library_df['author'].isin(['Arundhati Roy', 'Ruskin Bond'])]\n",
        "print(specific_authors[['title', 'author', 'category', 'year']])\n",
        "print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vm8QwHHa9Khe",
        "outputId": "48170683-0db4-41fb-a7c2-c7807654e215"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Books by Arundhati Roy or Ruskin Bond:\n",
            "                title         author      category  year\n",
            "1  Data Science Guide  Arundhati Roy  Data Science  2021\n",
            "3    Machine Learning    Ruskin Bond            ML  2022\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Bonus: Complex filtering\n",
        "print(\"Recent Data Science/ML books that are available:\")\n",
        "complex_filter = library_df[\n",
        "    (library_df['year'] >= 2021) &\n",
        "    (library_df['category'].isin(['Data Science', 'ML'])) &\n",
        "    (library_df['available'] == True)\n",
        "]\n",
        "print(complex_filter[['title', 'category', 'year']])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DdjItaLi9MaB",
        "outputId": "118aa260-f89c-4c39-854b-766b56c1faff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Recent Data Science/ML books that are available:\n",
            "              title category  year\n",
            "3  Machine Learning       ML  2022\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**🎯 ACTIVITY 3: CUSTOMER ORDER ANALYSIS**  \n",
        "**Topic**: Joining DataFrames  \n",
        "**Duration**: 15 minutes  \n",
        "**Difficulty**: Intermediate  \n",
        "## **Problem Statement**  \n",
        "An online store has separate datasets for customers and their orders. Join these datasets to analyze customer purchasing behavior and identify valuable customers.  \n",
        "Sample Data  \n",
        "customers_data = {\n",
        "    'customer_id': [1, 2, 3, 4, 5],\n",
        "    'name': ['Rajesh Kumar', 'Meena Roy', 'Amitabh Verma', 'Lakshmi Iyer', 'Sanjay Patel'],\n",
        "    'city': ['New Delhi', 'Bengaluru', 'Ahmedabad', 'Chennai', 'Mumbai'],\n",
        "    'signup_date': ['2023-01-15', '2023-02-20', '2023-01-30', '2023-03-10', '2023-02-05']\n",
        "}  \n",
        "orders_data = {\n",
        "    'order_id': [101, 102, 103, 104, 105, 106],\n",
        "    'customer_id': [1, 2, 1, 3, 6, 2],  # Note: customer_id 6 doesn't exist in customers\n",
        "    'product': ['Laptop', 'Phone', 'Tablet', 'Headphones', 'Camera', 'Watch'],\n",
        "    'amount': [1200, 800, 600, 150, 900, 300],\n",
        "    'order_date': ['2023-04-01', '2023-04-02', '2023-04-05', '2023-04-03', '2023-04-06', '2023-04-07']\n",
        "}\n",
        "  \n",
        "**Your Tasks**  \n",
        "**Task 1:** Perform inner join to see customers with orders  \n",
        "**Task 2:** Perform left join to see all customers (even those without orders)  \n",
        "**Task 3:** Calculate total spending per customer  \n",
        "**Task 4:** Find the customer with highest total spending  "
      ],
      "metadata": {
        "id": "OQDX7Gk699WY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Given data\n",
        "customers_data = {\n",
        "    'customer_id': [1, 2, 3, 4, 5],\n",
        "    'name': ['Rajesh Kumar', 'Meena Roy', 'Amitabh Verma', 'Lakshmi Iyer', 'Sanjay Patel'],\n",
        "    'city': ['New Delhi', 'Bengaluru', 'Ahmedabad', 'Chennai', 'Mumbai'],\n",
        "    'signup_date': ['2023-01-15', '2023-02-20', '2023-01-30', '2023-03-10', '2023-02-05']\n",
        "}\n",
        "\n",
        "orders_data = {\n",
        "    'order_id': [101, 102, 103, 104, 105, 106],\n",
        "    'customer_id': [1, 2, 1, 3, 6, 2],  # Note: customer_id 6 doesn't exist in customers\n",
        "    'product': ['Laptop', 'Phone', 'Tablet', 'Headphones', 'Camera', 'Watch'],\n",
        "    'amount': [1200, 800, 600, 150, 900, 300],\n",
        "    'order_date': ['2023-04-01', '2023-04-02', '2023-04-05', '2023-04-03', '2023-04-06', '2023-04-07']\n",
        "}"
      ],
      "metadata": {
        "id": "L9Ywp3ht9PSz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create DataFrames\n",
        "customers_df = pd.DataFrame(customers_data)\n",
        "orders_df = pd.DataFrame(orders_data)\n",
        "\n",
        "print(\"Customers DataFrame:\")\n",
        "print(customers_df)\n",
        "print(\"\\nOrders DataFrame:\")\n",
        "print(orders_df)\n",
        "print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P2aMyDnC-fNh",
        "outputId": "2f8ba118-9192-4bc7-88c7-a69fe844db9f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Customers DataFrame:\n",
            "   customer_id           name       city signup_date\n",
            "0            1   Rajesh Kumar  New Delhi  2023-01-15\n",
            "1            2      Meena Roy  Bengaluru  2023-02-20\n",
            "2            3  Amitabh Verma  Ahmedabad  2023-01-30\n",
            "3            4   Lakshmi Iyer    Chennai  2023-03-10\n",
            "4            5   Sanjay Patel     Mumbai  2023-02-05\n",
            "\n",
            "Orders DataFrame:\n",
            "   order_id  customer_id     product  amount  order_date\n",
            "0       101            1      Laptop    1200  2023-04-01\n",
            "1       102            2       Phone     800  2023-04-02\n",
            "2       103            1      Tablet     600  2023-04-05\n",
            "3       104            3  Headphones     150  2023-04-03\n",
            "4       105            6      Camera     900  2023-04-06\n",
            "5       106            2       Watch     300  2023-04-07\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 1: Perform inner join to see customers with orders\n",
        "print(\"Task 1: Inner Join (Customers with Orders)\")\n",
        "inner_joined = pd.merge(customers_df, orders_df, on='customer_id', how='inner')\n",
        "print(inner_joined[['name', 'city', 'product', 'amount']])\n",
        "print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FG6K4_ka-hLk",
        "outputId": "59753b9b-4752-40ed-a02e-397995f0447d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Task 1: Inner Join (Customers with Orders)\n",
            "            name       city     product  amount\n",
            "0   Rajesh Kumar  New Delhi      Laptop    1200\n",
            "1   Rajesh Kumar  New Delhi      Tablet     600\n",
            "2      Meena Roy  Bengaluru       Phone     800\n",
            "3      Meena Roy  Bengaluru       Watch     300\n",
            "4  Amitabh Verma  Ahmedabad  Headphones     150\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 2: Perform left join to see all customers (even those without orders)\n",
        "print(\"Task 2: Left Join (All Customers)\")\n",
        "left_joined = pd.merge(customers_df, orders_df, on='customer_id', how='left')\n",
        "print(left_joined[['name', 'city', 'product', 'amount']])\n",
        "print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FcO5OP1l-jMX",
        "outputId": "36813cb1-42e3-4525-eda5-4c98f8655df2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Task 2: Left Join (All Customers)\n",
            "            name       city     product  amount\n",
            "0   Rajesh Kumar  New Delhi      Laptop  1200.0\n",
            "1   Rajesh Kumar  New Delhi      Tablet   600.0\n",
            "2      Meena Roy  Bengaluru       Phone   800.0\n",
            "3      Meena Roy  Bengaluru       Watch   300.0\n",
            "4  Amitabh Verma  Ahmedabad  Headphones   150.0\n",
            "5   Lakshmi Iyer    Chennai         NaN     NaN\n",
            "6   Sanjay Patel     Mumbai         NaN     NaN\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 3: Calculate total spending per customer\n",
        "print(\"Task 3: Total Spending per Customer\")\n",
        "customer_spending = inner_joined.groupby(['customer_id', 'name']).agg({\n",
        "    'amount': 'sum',\n",
        "    'order_id': 'count'\n",
        "}).round(2)\n",
        "customer_spending.columns = ['total_spent', 'number_of_orders']\n",
        "print(customer_spending)\n",
        "print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QMBIRPKv-l9c",
        "outputId": "fc9d7f56-a356-4cb1-8d6e-e1a28774d654"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Task 3: Total Spending per Customer\n",
            "                           total_spent  number_of_orders\n",
            "customer_id name                                        \n",
            "1           Rajesh Kumar          1800                 2\n",
            "2           Meena Roy             1100                 2\n",
            "3           Amitabh Verma          150                 1\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 4: Find the customer with highest total spending\n",
        "print(\"Task 4: Customer with Highest Total Spending\")\n",
        "highest_spender = customer_spending.loc[customer_spending['total_spent'].idxmax()]\n",
        "print(f\"Highest spending customer: {highest_spender.name[1]}\")\n",
        "print(f\"Total amount: Rs. {highest_spender['total_spent']}\")\n",
        "print(f\"Number of orders: {highest_spender['number_of_orders']}\")\n",
        "print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WKSpRSIs-nrt",
        "outputId": "06180be6-c76d-49c7-cbbe-b06050569572"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Task 4: Customer with Highest Total Spending\n",
            "Highest spending customer: Rajesh Kumar\n",
            "Total amount: Rs. 1800\n",
            "Number of orders: 2\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Bonus: Show customers without any orders\n",
        "print(\"Bonus: Customers Without Orders\")\n",
        "no_orders = left_joined[left_joined['order_id'].isnull()]\n",
        "print(no_orders[['name', 'city']])\n",
        "print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vSM91zCk-p3R",
        "outputId": "c3e286a8-ef52-4ec9-c786-73d37e9e546d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bonus: Customers Without Orders\n",
            "           name     city\n",
            "5  Lakshmi Iyer  Chennai\n",
            "6  Sanjay Patel   Mumbai\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Bonus: Right join to see orders from unknown customers\n",
        "print(\"Bonus: Right Join (All Orders, including from unknown customers)\")\n",
        "right_joined = pd.merge(customers_df, orders_df, on='customer_id', how='right')\n",
        "unknown_customer_orders = right_joined[right_joined['name'].isnull()]\n",
        "print(\"Orders from unknown customers:\")\n",
        "print(unknown_customer_orders[['customer_id', 'product', 'amount']])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mlTKQcAP-0Q-",
        "outputId": "ce04e7d4-3cf7-4ca7-ad4b-cb3637327c3d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bonus: Right Join (All Orders, including from unknown customers)\n",
            "Orders from unknown customers:\n",
            "   customer_id product  amount\n",
            "4            6  Camera     900\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**🎯 ACTIVITY 4: EMPLOYEE SALARY DATA CLEANUP**  \n",
        "**Topic**: Handling Missing Data  \n",
        "**Duration**: 12 minutes  \n",
        "**Difficulty**: Intermediate  \n",
        "## **Problem Statement**  \n",
        "The HR department has an employee dataset with missing information. Clean the data by handling missing values appropriately using different strategies based on the column type and business logic.  \n",
        "**Sample Data (with intentional missing values)**  \n",
        "import numpy as np\n",
        "\n",
        "employee_data = {\n",
        "    'employee_id': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
        "    'name': ['Aarav', 'Riya', np.nan, 'Priya', 'Abdullah', 'Neha', 'Grace', np.nan, 'Suresh', 'Rohit'],\n",
        "    'department': ['IT', 'HR', 'IT', np.nan, 'Finance', 'IT', 'HR', 'Finance', np.nan, 'HR'],\n",
        "    'salary': [75000, np.nan, 82000, 65000, np.nan, 71000, 69000, 78000, 73000, np.nan],\n",
        "    'experience': [5, 8, np.nan, 6, 4, np.nan, 3, 9, 7, 2],\n",
        "    'performance_rating': [8.5, np.nan, 9.1, 7.8, 8.8, 8.2, np.nan, 8.9, 7.5, 9.0]\n",
        "}\n",
        "  \n",
        "**Your Tasks**  \n",
        "**Task 1:** Identify missing values in each column  \n",
        "**Task 2:** Fill missing salaries with department average  \n",
        "**Task 3:** Fill missing departments with mode (most frequent)  \n",
        "**Task 4:** Handle missing experience based on performance rating  "
      ],
      "metadata": {
        "id": "3cEoqWs4_P_k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Given data with missing values\n",
        "employee_data = {\n",
        "    'employee_id': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
        "     'name': ['Aarav', 'Riya', np.nan, 'Priya', 'Abdullah', 'Neha', 'Grace', np.nan, 'Suresh', 'Rohit'],\n",
        "    'department': ['IT', 'HR', 'IT', np.nan, 'Finance', 'IT', 'HR', 'Finance', np.nan, 'HR'],\n",
        "    'salary': [75000, np.nan, 82000, 65000, np.nan, 71000, 69000, 78000, 73000, np.nan],\n",
        "    'experience': [5, 8, np.nan, 6, 4, np.nan, 3, 9, 7, 2],\n",
        "    'performance_rating': [8.5, np.nan, 9.1, 7.8, 8.8, 8.2, np.nan, 8.9, 7.5, 9.0]\n",
        "}"
      ],
      "metadata": {
        "id": "C0qSIEO0-25a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create DataFrame\n",
        "employee_df = pd.DataFrame(employee_data)\n",
        "\n",
        "print(\"Original Data with Missing Values:\")\n",
        "print(employee_df)\n",
        "print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cak4jzDpAC3B",
        "outputId": "cc6d195f-ec22-43df-fdfb-abf20e7e812b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Data with Missing Values:\n",
            "   employee_id      name department   salary  experience  performance_rating\n",
            "0            1     Aarav         IT  75000.0         5.0                 8.5\n",
            "1            2      Riya         HR      NaN         8.0                 NaN\n",
            "2            3       NaN         IT  82000.0         NaN                 9.1\n",
            "3            4     Priya        NaN  65000.0         6.0                 7.8\n",
            "4            5  Abdullah    Finance      NaN         4.0                 8.8\n",
            "5            6      Neha         IT  71000.0         NaN                 8.2\n",
            "6            7     Grace         HR  69000.0         3.0                 NaN\n",
            "7            8       NaN    Finance  78000.0         9.0                 8.9\n",
            "8            9    Suresh        NaN  73000.0         7.0                 7.5\n",
            "9           10     Rohit         HR      NaN         2.0                 9.0\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 1: Identify missing values in each column\n",
        "print(\"Task 1: Missing Values Analysis\")\n",
        "missing_summary = employee_df.isnull().sum()\n",
        "missing_percentage = (missing_summary / len(employee_df) * 100).round(1)\n",
        "\n",
        "missing_analysis = pd.DataFrame({\n",
        "    'Missing Count': missing_summary,\n",
        "    'Missing Percentage': missing_percentage\n",
        "})\n",
        "\n",
        "print(missing_analysis)\n",
        "print()\n",
        "\n",
        "# Create a copy for cleaning\n",
        "employee_clean = employee_df.copy()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P5JdqSP6AFHC",
        "outputId": "56d47a96-c686-4726-ff47-505eb750ca57"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Task 1: Missing Values Analysis\n",
            "                    Missing Count  Missing Percentage\n",
            "employee_id                     0                 0.0\n",
            "name                            2                20.0\n",
            "department                      2                20.0\n",
            "salary                          3                30.0\n",
            "experience                      2                20.0\n",
            "performance_rating              2                20.0\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 2: Fill missing salaries with department average\n",
        "print(\"Task 2: Filling Missing Salaries with Department Average\")\n",
        "print(\"Before filling salaries:\")\n",
        "print(employee_clean[['name', 'department', 'salary']].head(6))\n",
        "\n",
        "employee_clean['salary'] = employee_clean.groupby('department')['salary'].transform(\n",
        "    lambda x: x.fillna(x.mean())\n",
        ")\n",
        "\n",
        "# For any remaining NaN salaries (if department is also missing), use overall mean\n",
        "overall_salary_mean = employee_clean['salary'].mean()\n",
        "employee_clean['salary'] = employee_clean['salary'].fillna(overall_salary_mean)\n",
        "\n",
        "print(\"\\nAfter filling salaries:\")\n",
        "print(employee_clean[['name', 'department', 'salary']].head(6))\n",
        "print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hoWsqtUHAJUn",
        "outputId": "78a797e3-feca-4706-e957-42e1f1f81fc0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Task 2: Filling Missing Salaries with Department Average\n",
            "Before filling salaries:\n",
            "       name department   salary\n",
            "0     Aarav         IT  75000.0\n",
            "1      Riya         HR      NaN\n",
            "2       NaN         IT  82000.0\n",
            "3     Priya        NaN  65000.0\n",
            "4  Abdullah    Finance      NaN\n",
            "5      Neha         IT  71000.0\n",
            "\n",
            "After filling salaries:\n",
            "       name department   salary\n",
            "0     Aarav         IT  75000.0\n",
            "1      Riya         HR  69000.0\n",
            "2       NaN         IT  82000.0\n",
            "3     Priya        NaN  73875.0\n",
            "4  Abdullah    Finance  78000.0\n",
            "5      Neha         IT  71000.0\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 3: Fill missing departments with mode (most frequent)\n",
        "print(\"Task 3: Filling Missing Departments with Mode\")\n",
        "print(\"Department value counts:\")\n",
        "print(employee_clean['department'].value_counts())\n",
        "\n",
        "department_mode = employee_clean['department'].mode()[0]\n",
        "print(f\"\\nMost frequent department: {department_mode}\")\n",
        "\n",
        "employee_clean['department'] = employee_clean['department'].fillna(department_mode)\n",
        "print(\"\\nAfter filling departments:\")\n",
        "print(employee_clean[['name', 'department']])\n",
        "print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xZcNT27JAL-c",
        "outputId": "934b58af-0c38-4523-bcd2-b85028a3e923"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Task 3: Filling Missing Departments with Mode\n",
            "Department value counts:\n",
            "department\n",
            "IT         3\n",
            "HR         3\n",
            "Finance    2\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Most frequent department: HR\n",
            "\n",
            "After filling departments:\n",
            "       name department\n",
            "0     Aarav         IT\n",
            "1      Riya         HR\n",
            "2       NaN         IT\n",
            "3     Priya         HR\n",
            "4  Abdullah    Finance\n",
            "5      Neha         IT\n",
            "6     Grace         HR\n",
            "7       NaN    Finance\n",
            "8    Suresh         HR\n",
            "9     Rohit         HR\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 4: Handle missing experience based on performance rating\n",
        "print(\"Task 4: Handling Missing Experience\")\n",
        "print(\"Missing experience analysis:\")\n",
        "missing_exp_data = employee_clean[employee_clean['experience'].isnull()]\n",
        "print(missing_exp_data[['name', 'performance_rating', 'experience']])\n",
        "\n",
        "# Strategy: Use correlation between performance and experience\n",
        "# Higher performance -> Higher experience (simplified assumption)\n",
        "def estimate_experience(row):\n",
        "    if pd.isnull(row['experience']):\n",
        "        if pd.notnull(row['performance_rating']):\n",
        "            # Simple formula: experience = performance_rating * 1.2 (rounded)\n",
        "            return round(row['performance_rating'] * 1.2)\n",
        "        else:\n",
        "            # If both are missing, use median experience\n",
        "            return employee_clean['experience'].median()\n",
        "    return row['experience']\n",
        "\n",
        "employee_clean['experience'] = employee_clean.apply(estimate_experience, axis=1)\n",
        "\n",
        "# Handle missing performance ratings with median\n",
        "median_performance = employee_clean['performance_rating'].median()\n",
        "employee_clean['performance_rating'] = employee_clean['performance_rating'].fillna(median_performance)\n",
        "\n",
        "# Handle missing names\n",
        "employee_clean['name'] = employee_clean['name'].fillna('Unknown Employee')\n",
        "\n",
        "print(\"\\nFinal cleaned dataset:\")\n",
        "print(employee_clean)\n",
        "print()\n",
        "\n",
        "# Verification: Check for any remaining missing values\n",
        "print(\"Final Missing Values Check:\")\n",
        "final_missing = employee_clean.isnull().sum()\n",
        "print(final_missing)\n",
        "print()\n",
        "\n",
        "if final_missing.sum() == 0:\n",
        "    print(\"✅ Success! All missing values have been handled.\")\n",
        "else:\n",
        "    print(\"❌ Some missing values still remain.\")\n",
        "\n",
        "# Summary of cleaning actions\n",
        "print(\"\\nSummary of Data Cleaning Actions:\")\n",
        "print(\"1. Salaries: Filled with department average, then overall average\")\n",
        "print(\"2. Departments: Filled with mode (most frequent)\")\n",
        "print(\"3. Experience: Estimated based on performance rating\")\n",
        "print(\"4. Performance Rating: Filled with median\")\n",
        "print(\"5. Names: Filled with 'Unknown Employee'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d77hxfHhAOb0",
        "outputId": "549bce85-130a-4e71-a188-d58307f2c022"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Task 4: Handling Missing Experience\n",
            "Missing experience analysis:\n",
            "   name  performance_rating  experience\n",
            "2   NaN                 9.1         NaN\n",
            "5  Neha                 8.2         NaN\n",
            "\n",
            "Final cleaned dataset:\n",
            "   employee_id              name department   salary  experience  \\\n",
            "0            1             Aarav         IT  75000.0         5.0   \n",
            "1            2              Riya         HR  69000.0         8.0   \n",
            "2            3  Unknown Employee         IT  82000.0        11.0   \n",
            "3            4             Priya         HR  73875.0         6.0   \n",
            "4            5          Abdullah    Finance  78000.0         4.0   \n",
            "5            6              Neha         IT  71000.0        10.0   \n",
            "6            7             Grace         HR  69000.0         3.0   \n",
            "7            8  Unknown Employee    Finance  78000.0         9.0   \n",
            "8            9            Suresh         HR  73875.0         7.0   \n",
            "9           10             Rohit         HR  69000.0         2.0   \n",
            "\n",
            "   performance_rating  \n",
            "0                8.50  \n",
            "1                8.65  \n",
            "2                9.10  \n",
            "3                7.80  \n",
            "4                8.80  \n",
            "5                8.20  \n",
            "6                8.65  \n",
            "7                8.90  \n",
            "8                7.50  \n",
            "9                9.00  \n",
            "\n",
            "Final Missing Values Check:\n",
            "employee_id           0\n",
            "name                  0\n",
            "department            0\n",
            "salary                0\n",
            "experience            0\n",
            "performance_rating    0\n",
            "dtype: int64\n",
            "\n",
            "✅ Success! All missing values have been handled.\n",
            "\n",
            "Summary of Data Cleaning Actions:\n",
            "1. Salaries: Filled with department average, then overall average\n",
            "2. Departments: Filled with mode (most frequent)\n",
            "3. Experience: Estimated based on performance rating\n",
            "4. Performance Rating: Filled with median\n",
            "5. Names: Filled with 'Unknown Employee'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**🎯 ACTIVITY 5: SALES PERFORMANCE DASHBOARD**  \n",
        "**Topic**: Grouping and Aggregation  \n",
        "**Duration**: 15 minutes  \n",
        "**Difficulty**: Intermediate-Advanced  \n",
        "## **Problem Statement**  \n",
        "A retail company wants to analyze sales performance across different regions, products, and salespeople. Use groupby operations to create comprehensive sales insights for management decisions.  \n",
        "**Sample Data**  \n",
        "sales_data = {\n",
        "    'sale_id': range(1, 21),\n",
        "        'salesperson': ['Aarav', 'Riya', 'Rohan', 'Ananya', 'Vikram', 'Priya', 'Suresh', 'Kavya', 'Neha', 'Sanjay',\n",
        "'Lakshmi', 'Manish', 'Deepa', 'Arjun', 'Meera', 'Rakesh', 'Pooja', 'Varun', 'Sindhu', 'Rahul'],\n",
        "    'region': ['North', 'South', 'East', 'West'] * 5,\n",
        "    'product_category': ['Electronics', 'Clothing', 'Books', 'Home',\n",
        "                        'Electronics', 'Clothing', 'Books', 'Home',\n",
        "                        'Electronics', 'Clothing', 'Books', 'Home',\n",
        "                        'Electronics', 'Clothing', 'Books', 'Home',\n",
        "                        'Electronics', 'Clothing', 'Books', 'Home'],\n",
        "    'sales_amount': [1200, 800, 300, 600, 1500, 900, 250, 700,\n",
        "                    1300, 850, 320, 650, 1100, 750, 280, 580,\n",
        "                    1400, 820, 300, 620],\n",
        "    'units_sold': [3, 8, 5, 2, 4, 9, 4, 3, 3, 7, 6, 2, 2, 6, 3, 2, 4, 8, 5, 3],\n",
        "    'quarter': ['Q1', 'Q1', 'Q1', 'Q1', 'Q2', 'Q2', 'Q2', 'Q2',\n",
        "               'Q3', 'Q3', 'Q3', 'Q3', 'Q4', 'Q4', 'Q4', 'Q4',\n",
        "               'Q1', 'Q2', 'Q3', 'Q4']\n",
        "}\n",
        "  \n",
        "**Your Tasks**  \n",
        "**Task 1:** Calculate total sales by salesperson  \n",
        "**Task 2:** Find average sales amount by region and product category  \n",
        "**Task 3:** Identify top performing quarter for each region  \n",
        "**Task 4:** Create a comprehensive performance summary with custom metrics  "
      ],
      "metadata": {
        "id": "qN3pwh39BLjW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Given sales data\n",
        "sales_data = {\n",
        "    'sale_id': range(1, 21),\n",
        "        'salesperson': ['Aarav', 'Riya', 'Rohan', 'Ananya', 'Vikram', 'Priya', 'Suresh', 'Kavya', 'Neha', 'Sanjay',\n",
        "'Lakshmi', 'Manish', 'Deepa', 'Arjun', 'Meera', 'Rakesh', 'Pooja', 'Varun', 'Sindhu', 'Rahul'],\n",
        "    'region': ['North', 'South', 'East', 'West'] * 5,\n",
        "    'product_category': ['Electronics', 'Clothing', 'Books', 'Home',\n",
        "                        'Electronics', 'Clothing', 'Books', 'Home',\n",
        "                        'Electronics', 'Clothing', 'Books', 'Home',\n",
        "                        'Electronics', 'Clothing', 'Books', 'Home',\n",
        "                        'Electronics', 'Clothing', 'Books', 'Home'],\n",
        "    'sales_amount': [1200, 800, 300, 600, 1500, 900, 250, 700,\n",
        "                    1300, 850, 320, 650, 1100, 750, 280, 580,\n",
        "                    1400, 820, 300, 620],\n",
        "    'units_sold': [3, 8, 5, 2, 4, 9, 4, 3, 3, 7, 6, 2, 2, 6, 3, 2, 4, 8, 5, 3],\n",
        "    'quarter': ['Q1', 'Q1', 'Q1', 'Q1', 'Q2', 'Q2', 'Q2', 'Q2',\n",
        "               'Q3', 'Q3', 'Q3', 'Q3', 'Q4', 'Q4', 'Q4', 'Q4',\n",
        "               'Q1', 'Q2', 'Q3', 'Q4']\n",
        "}"
      ],
      "metadata": {
        "id": "53abK0fNAT5C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create DataFrame\n",
        "sales_df = pd.DataFrame(sales_data)\n",
        "\n",
        "print(\"Sales Dataset Overview:\")\n",
        "print(sales_df.head(8))\n",
        "print(f\"\\nDataset shape: {sales_df.shape}\")\n",
        "print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cRPO_J1wBvqC",
        "outputId": "cd886f11-5cc1-4da6-e521-faed73731a46"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sales Dataset Overview:\n",
            "   sale_id salesperson region product_category  sales_amount  units_sold  \\\n",
            "0        1       Aarav  North      Electronics          1200           3   \n",
            "1        2        Riya  South         Clothing           800           8   \n",
            "2        3       Rohan   East            Books           300           5   \n",
            "3        4      Ananya   West             Home           600           2   \n",
            "4        5      Vikram  North      Electronics          1500           4   \n",
            "5        6       Priya  South         Clothing           900           9   \n",
            "6        7      Suresh   East            Books           250           4   \n",
            "7        8       Kavya   West             Home           700           3   \n",
            "\n",
            "  quarter  \n",
            "0      Q1  \n",
            "1      Q1  \n",
            "2      Q1  \n",
            "3      Q1  \n",
            "4      Q2  \n",
            "5      Q2  \n",
            "6      Q2  \n",
            "7      Q2  \n",
            "\n",
            "Dataset shape: (20, 7)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 1: Calculate total sales by salesperson\n",
        "print(\"Task 1: Total Sales by Salesperson\")\n",
        "salesperson_performance = sales_df.groupby('salesperson').agg({\n",
        "    'sales_amount': ['sum', 'mean', 'count'],\n",
        "    'units_sold': 'sum'\n",
        "}).round(2)\n",
        "\n",
        "# Flatten column names\n",
        "salesperson_performance.columns = ['total_sales', 'avg_sales', 'num_transactions', 'total_units']\n",
        "\n",
        "# Sort by total sales\n",
        "salesperson_performance = salesperson_performance.sort_values('total_sales', ascending=False)\n",
        "print(salesperson_performance)\n",
        "print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a3ihMbEFBxl_",
        "outputId": "b4ecd930-b3d1-4c65-e192-3e423e3b0dcd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Task 1: Total Sales by Salesperson\n",
            "             total_sales  avg_sales  num_transactions  total_units\n",
            "salesperson                                                       \n",
            "Vikram              1500     1500.0                 1            4\n",
            "Pooja               1400     1400.0                 1            4\n",
            "Neha                1300     1300.0                 1            3\n",
            "Aarav               1200     1200.0                 1            3\n",
            "Deepa               1100     1100.0                 1            2\n",
            "Priya                900      900.0                 1            9\n",
            "Sanjay               850      850.0                 1            7\n",
            "Varun                820      820.0                 1            8\n",
            "Riya                 800      800.0                 1            8\n",
            "Arjun                750      750.0                 1            6\n",
            "Kavya                700      700.0                 1            3\n",
            "Manish               650      650.0                 1            2\n",
            "Rahul                620      620.0                 1            3\n",
            "Ananya               600      600.0                 1            2\n",
            "Rakesh               580      580.0                 1            2\n",
            "Lakshmi              320      320.0                 1            6\n",
            "Sindhu               300      300.0                 1            5\n",
            "Rohan                300      300.0                 1            5\n",
            "Meera                280      280.0                 1            3\n",
            "Suresh               250      250.0                 1            4\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 2: Find average sales amount by region and product category\n",
        "print(\"Task 2: Average Sales by Region and Product Category\")\n",
        "region_category_avg = sales_df.groupby(['region', 'product_category'])['sales_amount'].mean().round(2)\n",
        "print(\"Average Sales Amount:\")\n",
        "print(region_category_avg)\n",
        "print()\n",
        "\n",
        "# Pivot table for better visualization\n",
        "pivot_table = sales_df.pivot_table(\n",
        "    values='sales_amount',\n",
        "    index='region',\n",
        "    columns='product_category',\n",
        "    aggfunc='mean'\n",
        ").round(2)\n",
        "\n",
        "print(\"Pivot Table - Average Sales by Region x Product Category:\")\n",
        "print(pivot_table)\n",
        "print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s553yWvMB0Ed",
        "outputId": "a92535cb-080e-49e2-982c-5569ce183017"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Task 2: Average Sales by Region and Product Category\n",
            "Average Sales Amount:\n",
            "region  product_category\n",
            "East    Books                290.0\n",
            "North   Electronics         1300.0\n",
            "South   Clothing             824.0\n",
            "West    Home                 630.0\n",
            "Name: sales_amount, dtype: float64\n",
            "\n",
            "Pivot Table - Average Sales by Region x Product Category:\n",
            "product_category  Books  Clothing  Electronics   Home\n",
            "region                                               \n",
            "East              290.0       NaN          NaN    NaN\n",
            "North               NaN       NaN       1300.0    NaN\n",
            "South               NaN     824.0          NaN    NaN\n",
            "West                NaN       NaN          NaN  630.0\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 3: Identify top performing quarter for each region\n",
        "print(\"Task 3: Top Performing Quarter by Region\")\n",
        "quarterly_regional = sales_df.groupby(['region', 'quarter'])['sales_amount'].sum().reset_index()\n",
        "\n",
        "# Find max sales quarter for each region\n",
        "top_quarters = quarterly_regional.loc[quarterly_regional.groupby('region')['sales_amount'].idxmax()]\n",
        "print(top_quarters)\n",
        "print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HiZQlkBhB23U",
        "outputId": "4a3043e5-7b16-4e36-de01-600f3fc353ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Task 3: Top Performing Quarter by Region\n",
            "   region quarter  sales_amount\n",
            "2    East      Q3           620\n",
            "4   North      Q1          2600\n",
            "9   South      Q2          1720\n",
            "15   West      Q4          1200\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 4: Create a comprehensive performance summary with custom metrics\n",
        "print(\"Task 4: Comprehensive Performance Summary\")\n",
        "\n",
        "# Define custom aggregation functions\n",
        "def sales_range(x):\n",
        "    return x.max() - x.min()\n",
        "\n",
        "def efficiency_score(group):\n",
        "    # Custom metric: sales per unit\n",
        "    return (group['sales_amount'] / group['units_sold']).mean()\n",
        "\n",
        "# Multi-level grouping with custom metrics\n",
        "comprehensive_summary = sales_df.groupby('salesperson').apply(\n",
        "    lambda group: pd.Series({\n",
        "        'total_sales': group['sales_amount'].sum(),\n",
        "        'avg_sales_per_transaction': group['sales_amount'].mean(),\n",
        "        'total_transactions': len(group),\n",
        "        'total_units_sold': group['units_sold'].sum(),\n",
        "        'sales_efficiency': (group['sales_amount'] / group['units_sold']).mean(),\n",
        "        'sales_variance': group['sales_amount'].std(),\n",
        "        'best_quarter': group.groupby('quarter')['sales_amount'].sum().idxmax(),\n",
        "        'regions_covered': group['region'].nunique(),\n",
        "        'product_categories': group['product_category'].nunique()\n",
        "    })\n",
        ").round(2)\n",
        "\n",
        "print(comprehensive_summary)\n",
        "print()\n",
        "\n",
        "# Bonus: Advanced analysis with transform and apply\n",
        "print(\"Bonus: Advanced Analysis\")\n",
        "\n",
        "# Add performance rankings within each region\n",
        "sales_df['regional_rank'] = sales_df.groupby('region')['sales_amount'].rank(ascending=False)\n",
        "\n",
        "# Add percentage of total sales\n",
        "total_sales = sales_df['sales_amount'].sum()\n",
        "sales_df['pct_of_total_sales'] = (sales_df['sales_amount'] / total_sales * 100).round(2)\n",
        "\n",
        "# Add salesperson average for comparison\n",
        "sales_df['salesperson_avg'] = sales_df.groupby('salesperson')['sales_amount'].transform('mean')\n",
        "sales_df['vs_personal_avg'] = sales_df['sales_amount'] - sales_df['salesperson_avg']\n",
        "\n",
        "print(\"Enhanced dataset with derived metrics:\")\n",
        "print(sales_df[['salesperson', 'region', 'sales_amount', 'regional_rank',\n",
        "               'pct_of_total_sales', 'vs_personal_avg']].head(10))\n",
        "print()\n",
        "\n",
        "# Final insights\n",
        "print(\"Key Business Insights:\")\n",
        "best_salesperson = salesperson_performance.index[0]\n",
        "best_region_category = pivot_table.stack().idxmax()\n",
        "total_company_sales = sales_df['sales_amount'].sum()\n",
        "\n",
        "print(f\"1. Top Salesperson: {best_salesperson} with Rs. {salesperson_performance.loc[best_salesperson, 'total_sales']:,.2f}\")\n",
        "print(f\"2. Best Region-Category Combo: {best_region_category[0]} - {best_region_category[1]}\")\n",
        "print(f\"3. Total Company Sales: Rs. {total_company_sales:,.2f}\")\n",
        "print(f\"4. Average Transaction Value: Rs. {sales_df['sales_amount'].mean():.2f}\")\n",
        "print(f\"5. Most Consistent Performer: {comprehensive_summary['sales_variance'].idxmin()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AT3jZLSvB48d",
        "outputId": "62bcd519-ad08-434f-e428-35fb5b43d460"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Task 4: Comprehensive Performance Summary\n",
            "             total_sales  avg_sales_per_transaction  total_transactions  \\\n",
            "salesperson                                                               \n",
            "Aarav               1200                     1200.0                   1   \n",
            "Ananya               600                      600.0                   1   \n",
            "Arjun                750                      750.0                   1   \n",
            "Deepa               1100                     1100.0                   1   \n",
            "Kavya                700                      700.0                   1   \n",
            "Lakshmi              320                      320.0                   1   \n",
            "Manish               650                      650.0                   1   \n",
            "Meera                280                      280.0                   1   \n",
            "Neha                1300                     1300.0                   1   \n",
            "Pooja               1400                     1400.0                   1   \n",
            "Priya                900                      900.0                   1   \n",
            "Rahul                620                      620.0                   1   \n",
            "Rakesh               580                      580.0                   1   \n",
            "Riya                 800                      800.0                   1   \n",
            "Rohan                300                      300.0                   1   \n",
            "Sanjay               850                      850.0                   1   \n",
            "Sindhu               300                      300.0                   1   \n",
            "Suresh               250                      250.0                   1   \n",
            "Varun                820                      820.0                   1   \n",
            "Vikram              1500                     1500.0                   1   \n",
            "\n",
            "             total_units_sold  sales_efficiency  sales_variance best_quarter  \\\n",
            "salesperson                                                                    \n",
            "Aarav                       3            400.00             NaN           Q1   \n",
            "Ananya                      2            300.00             NaN           Q1   \n",
            "Arjun                       6            125.00             NaN           Q4   \n",
            "Deepa                       2            550.00             NaN           Q4   \n",
            "Kavya                       3            233.33             NaN           Q2   \n",
            "Lakshmi                     6             53.33             NaN           Q3   \n",
            "Manish                      2            325.00             NaN           Q3   \n",
            "Meera                       3             93.33             NaN           Q4   \n",
            "Neha                        3            433.33             NaN           Q3   \n",
            "Pooja                       4            350.00             NaN           Q1   \n",
            "Priya                       9            100.00             NaN           Q2   \n",
            "Rahul                       3            206.67             NaN           Q4   \n",
            "Rakesh                      2            290.00             NaN           Q4   \n",
            "Riya                        8            100.00             NaN           Q1   \n",
            "Rohan                       5             60.00             NaN           Q1   \n",
            "Sanjay                      7            121.43             NaN           Q3   \n",
            "Sindhu                      5             60.00             NaN           Q3   \n",
            "Suresh                      4             62.50             NaN           Q2   \n",
            "Varun                       8            102.50             NaN           Q2   \n",
            "Vikram                      4            375.00             NaN           Q2   \n",
            "\n",
            "             regions_covered  product_categories  \n",
            "salesperson                                       \n",
            "Aarav                      1                   1  \n",
            "Ananya                     1                   1  \n",
            "Arjun                      1                   1  \n",
            "Deepa                      1                   1  \n",
            "Kavya                      1                   1  \n",
            "Lakshmi                    1                   1  \n",
            "Manish                     1                   1  \n",
            "Meera                      1                   1  \n",
            "Neha                       1                   1  \n",
            "Pooja                      1                   1  \n",
            "Priya                      1                   1  \n",
            "Rahul                      1                   1  \n",
            "Rakesh                     1                   1  \n",
            "Riya                       1                   1  \n",
            "Rohan                      1                   1  \n",
            "Sanjay                     1                   1  \n",
            "Sindhu                     1                   1  \n",
            "Suresh                     1                   1  \n",
            "Varun                      1                   1  \n",
            "Vikram                     1                   1  \n",
            "\n",
            "Bonus: Advanced Analysis\n",
            "Enhanced dataset with derived metrics:\n",
            "  salesperson region  sales_amount  regional_rank  pct_of_total_sales  \\\n",
            "0       Aarav  North          1200            4.0                7.88   \n",
            "1        Riya  South           800            4.0                5.26   \n",
            "2       Rohan   East           300            2.5                1.97   \n",
            "3      Ananya   West           600            4.0                3.94   \n",
            "4      Vikram  North          1500            1.0                9.86   \n",
            "5       Priya  South           900            1.0                5.91   \n",
            "6      Suresh   East           250            5.0                1.64   \n",
            "7       Kavya   West           700            1.0                4.60   \n",
            "8        Neha  North          1300            3.0                8.54   \n",
            "9      Sanjay  South           850            2.0                5.58   \n",
            "\n",
            "   vs_personal_avg  \n",
            "0              0.0  \n",
            "1              0.0  \n",
            "2              0.0  \n",
            "3              0.0  \n",
            "4              0.0  \n",
            "5              0.0  \n",
            "6              0.0  \n",
            "7              0.0  \n",
            "8              0.0  \n",
            "9              0.0  \n",
            "\n",
            "Key Business Insights:\n",
            "1. Top Salesperson: Vikram with Rs. 1,500.00\n",
            "2. Best Region-Category Combo: North - Electronics\n",
            "3. Total Company Sales: Rs. 15,220.00\n",
            "4. Average Transaction Value: Rs. 761.00\n",
            "5. Most Consistent Performer: nan\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2554962081.py:13: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
            "  comprehensive_summary = sales_df.groupby('salesperson').apply(\n",
            "/tmp/ipython-input-2554962081.py:59: FutureWarning: The behavior of Series.idxmin with all-NA values, or any-NA and skipna=False, is deprecated. In a future version this will raise ValueError\n",
            "  print(f\"5. Most Consistent Performer: {comprehensive_summary['sales_variance'].idxmin()}\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 4: Create a comprehensive performance summary with custom metrics\n",
        "print(\"Task 4: Comprehensive Performance Summary\")\n",
        "import numpy as np  # Add numpy import for np.nan\n",
        "\n",
        "# Define custom aggregation functions\n",
        "def sales_range(x):\n",
        "    return x.max() - x.min()\n",
        "\n",
        "def efficiency_score(group):\n",
        "    # Custom metric: sales per unit\n",
        "    return (group['sales_amount'] / group['units_sold']).mean()\n",
        "\n",
        "# Multi-level grouping with custom metrics - Compatible version\n",
        "def create_summary_stats(group):\n",
        "    \"\"\"Create summary statistics for each salesperson group\"\"\"\n",
        "    # Calculate sales efficiency safely\n",
        "    if len(group) > 0 and (group['units_sold'] > 0).all():\n",
        "        sales_efficiency = (group['sales_amount'] / group['units_sold']).mean()\n",
        "    else:\n",
        "        sales_efficiency = np.nan\n",
        "\n",
        "    # Calculate sales variance safely\n",
        "    sales_variance = group['sales_amount'].std() if len(group) > 1 else 0.0\n",
        "\n",
        "    # Find best quarter safely\n",
        "    if 'quarter' in group.columns and len(group) > 0:\n",
        "        quarterly_sales = group.groupby('quarter')['sales_amount'].sum()\n",
        "        best_quarter = quarterly_sales.idxmax() if len(quarterly_sales) > 0 else 'N/A'\n",
        "    else:\n",
        "        best_quarter = 'N/A'\n",
        "\n",
        "    return pd.Series({\n",
        "        'total_sales': group['sales_amount'].sum(),\n",
        "        'avg_sales_per_transaction': group['sales_amount'].mean(),\n",
        "        'total_transactions': len(group),\n",
        "        'total_units_sold': group['units_sold'].sum(),\n",
        "        'sales_efficiency': sales_efficiency,\n",
        "        'sales_variance': sales_variance,\n",
        "        'best_quarter': best_quarter,\n",
        "        'regions_covered': group['region'].nunique(),\n",
        "        'product_categories': group['product_category'].nunique()\n",
        "    })\n",
        "\n",
        "# Handle different pandas versions gracefully\n",
        "try:\n",
        "    # For pandas 2.0+ (with include_groups parameter)\n",
        "    comprehensive_summary = sales_df.groupby('salesperson').apply(create_summary_stats, include_groups=False).round(2)\n",
        "except TypeError:\n",
        "    # For older pandas versions (without include_groups parameter)\n",
        "    import warnings\n",
        "    with warnings.catch_warnings():\n",
        "        warnings.simplefilter(\"ignore\", DeprecationWarning)\n",
        "        comprehensive_summary = sales_df.groupby('salesperson').apply(create_summary_stats).round(2)\n",
        "\n",
        "print(comprehensive_summary)\n",
        "print()\n",
        "\n",
        "# Bonus: Advanced analysis with transform and apply\n",
        "print(\"Bonus: Advanced Analysis\")\n",
        "\n",
        "# Add performance rankings within each region\n",
        "sales_df['regional_rank'] = sales_df.groupby('region')['sales_amount'].rank(ascending=False)\n",
        "\n",
        "# Add percentage of total sales\n",
        "total_sales = sales_df['sales_amount'].sum()\n",
        "sales_df['pct_of_total_sales'] = (sales_df['sales_amount'] / total_sales * 100).round(2)\n",
        "\n",
        "# Add salesperson average for comparison\n",
        "sales_df['salesperson_avg'] = sales_df.groupby('salesperson')['sales_amount'].transform('mean')\n",
        "sales_df['vs_personal_avg'] = sales_df['sales_amount'] - sales_df['salesperson_avg']\n",
        "\n",
        "print(\"Enhanced dataset with derived metrics:\")\n",
        "print(sales_df[['salesperson', 'region', 'sales_amount', 'regional_rank',\n",
        "               'pct_of_total_sales', 'vs_personal_avg']].head(10))\n",
        "print()\n",
        "\n",
        "# Final insights - Fixed version\n",
        "print(\"Key Business Insights:\")\n",
        "best_salesperson = salesperson_performance.index[0]\n",
        "best_region_category = pivot_table.stack().idxmax()\n",
        "total_company_sales = sales_df['sales_amount'].sum()\n",
        "\n",
        "print(f\"1. Top Salesperson: {best_salesperson} with Rs. {salesperson_performance.loc[best_salesperson, 'total_sales']:,.2f}\")\n",
        "print(f\"2. Best Region-Category Combo: {best_region_category[0]} - {best_region_category[1]}\")\n",
        "print(f\"3. Total Company Sales: Rs. {total_company_sales:,.2f}\")\n",
        "print(f\"4. Average Transaction Value: Rs. {sales_df['sales_amount'].mean():.2f}\")\n",
        "\n",
        "# Fixed most consistent performer calculation\n",
        "valid_variance = comprehensive_summary['sales_variance'].dropna()\n",
        "if len(valid_variance) > 0 and not valid_variance.isna().all():\n",
        "    most_consistent = valid_variance.idxmin()\n",
        "    consistency_score = valid_variance.min()\n",
        "    print(f\"5. Most Consistent Performer: {most_consistent} (Variance: {consistency_score:.2f})\")\n",
        "else:\n",
        "    print(\"5. Most Consistent Performer: Insufficient data for variance calculation\")\n",
        "\n",
        "# Additional insights with better error handling\n",
        "print(\"\\nAdditional Performance Metrics:\")\n",
        "print(f\"6. Total Salespersons: {len(comprehensive_summary)}\")\n",
        "print(f\"7. Average Sales per Salesperson: Rs. {comprehensive_summary['total_sales'].mean():,.2f}\")\n",
        "\n",
        "# Show variance statistics\n",
        "variance_stats = comprehensive_summary['sales_variance'].describe()\n",
        "print(f\"8. Sales Variance Statistics:\")\n",
        "print(f\"   - Mean Variance: {variance_stats['mean']:.2f}\")\n",
        "print(f\"   - Min Variance: {variance_stats['min']:.2f}\")\n",
        "print(f\"   - Max Variance: {variance_stats['max']:.2f}\")\n",
        "\n",
        "# Alternative consistency measure using coefficient of variation\n",
        "comprehensive_summary['cv'] = (comprehensive_summary['sales_variance'] / comprehensive_summary['avg_sales_per_transaction']) * 100\n",
        "valid_cv = comprehensive_summary['cv'].dropna()\n",
        "if len(valid_cv) > 0:\n",
        "    most_consistent_cv = valid_cv.idxmin()\n",
        "    print(f\"9. Most Consistent (by CV): {most_consistent_cv} (CV: {valid_cv.min():.2f}%)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GHdnMoCFDGw7",
        "outputId": "62952fce-bad7-4e7e-9ddc-fedb5ee271cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Task 4: Comprehensive Performance Summary\n",
            "             total_sales  avg_sales_per_transaction  total_transactions  \\\n",
            "salesperson                                                               \n",
            "Aarav               1200                     1200.0                   1   \n",
            "Ananya               600                      600.0                   1   \n",
            "Arjun                750                      750.0                   1   \n",
            "Deepa               1100                     1100.0                   1   \n",
            "Kavya                700                      700.0                   1   \n",
            "Lakshmi              320                      320.0                   1   \n",
            "Manish               650                      650.0                   1   \n",
            "Meera                280                      280.0                   1   \n",
            "Neha                1300                     1300.0                   1   \n",
            "Pooja               1400                     1400.0                   1   \n",
            "Priya                900                      900.0                   1   \n",
            "Rahul                620                      620.0                   1   \n",
            "Rakesh               580                      580.0                   1   \n",
            "Riya                 800                      800.0                   1   \n",
            "Rohan                300                      300.0                   1   \n",
            "Sanjay               850                      850.0                   1   \n",
            "Sindhu               300                      300.0                   1   \n",
            "Suresh               250                      250.0                   1   \n",
            "Varun                820                      820.0                   1   \n",
            "Vikram              1500                     1500.0                   1   \n",
            "\n",
            "             total_units_sold  sales_efficiency  sales_variance best_quarter  \\\n",
            "salesperson                                                                    \n",
            "Aarav                       3            400.00             0.0           Q1   \n",
            "Ananya                      2            300.00             0.0           Q1   \n",
            "Arjun                       6            125.00             0.0           Q4   \n",
            "Deepa                       2            550.00             0.0           Q4   \n",
            "Kavya                       3            233.33             0.0           Q2   \n",
            "Lakshmi                     6             53.33             0.0           Q3   \n",
            "Manish                      2            325.00             0.0           Q3   \n",
            "Meera                       3             93.33             0.0           Q4   \n",
            "Neha                        3            433.33             0.0           Q3   \n",
            "Pooja                       4            350.00             0.0           Q1   \n",
            "Priya                       9            100.00             0.0           Q2   \n",
            "Rahul                       3            206.67             0.0           Q4   \n",
            "Rakesh                      2            290.00             0.0           Q4   \n",
            "Riya                        8            100.00             0.0           Q1   \n",
            "Rohan                       5             60.00             0.0           Q1   \n",
            "Sanjay                      7            121.43             0.0           Q3   \n",
            "Sindhu                      5             60.00             0.0           Q3   \n",
            "Suresh                      4             62.50             0.0           Q2   \n",
            "Varun                       8            102.50             0.0           Q2   \n",
            "Vikram                      4            375.00             0.0           Q2   \n",
            "\n",
            "             regions_covered  product_categories  \n",
            "salesperson                                       \n",
            "Aarav                      1                   1  \n",
            "Ananya                     1                   1  \n",
            "Arjun                      1                   1  \n",
            "Deepa                      1                   1  \n",
            "Kavya                      1                   1  \n",
            "Lakshmi                    1                   1  \n",
            "Manish                     1                   1  \n",
            "Meera                      1                   1  \n",
            "Neha                       1                   1  \n",
            "Pooja                      1                   1  \n",
            "Priya                      1                   1  \n",
            "Rahul                      1                   1  \n",
            "Rakesh                     1                   1  \n",
            "Riya                       1                   1  \n",
            "Rohan                      1                   1  \n",
            "Sanjay                     1                   1  \n",
            "Sindhu                     1                   1  \n",
            "Suresh                     1                   1  \n",
            "Varun                      1                   1  \n",
            "Vikram                     1                   1  \n",
            "\n",
            "Bonus: Advanced Analysis\n",
            "Enhanced dataset with derived metrics:\n",
            "  salesperson region  sales_amount  regional_rank  pct_of_total_sales  \\\n",
            "0       Aarav  North          1200            4.0                7.88   \n",
            "1        Riya  South           800            4.0                5.26   \n",
            "2       Rohan   East           300            2.5                1.97   \n",
            "3      Ananya   West           600            4.0                3.94   \n",
            "4      Vikram  North          1500            1.0                9.86   \n",
            "5       Priya  South           900            1.0                5.91   \n",
            "6      Suresh   East           250            5.0                1.64   \n",
            "7       Kavya   West           700            1.0                4.60   \n",
            "8        Neha  North          1300            3.0                8.54   \n",
            "9      Sanjay  South           850            2.0                5.58   \n",
            "\n",
            "   vs_personal_avg  \n",
            "0              0.0  \n",
            "1              0.0  \n",
            "2              0.0  \n",
            "3              0.0  \n",
            "4              0.0  \n",
            "5              0.0  \n",
            "6              0.0  \n",
            "7              0.0  \n",
            "8              0.0  \n",
            "9              0.0  \n",
            "\n",
            "Key Business Insights:\n",
            "1. Top Salesperson: Vikram with Rs. 1,500.00\n",
            "2. Best Region-Category Combo: North - Electronics\n",
            "3. Total Company Sales: Rs. 15,220.00\n",
            "4. Average Transaction Value: Rs. 761.00\n",
            "5. Most Consistent Performer: Aarav (Variance: 0.00)\n",
            "\n",
            "Additional Performance Metrics:\n",
            "6. Total Salespersons: 20\n",
            "7. Average Sales per Salesperson: Rs. 761.00\n",
            "8. Sales Variance Statistics:\n",
            "   - Mean Variance: 0.00\n",
            "   - Min Variance: 0.00\n",
            "   - Max Variance: 0.00\n",
            "9. Most Consistent (by CV): Aarav (CV: 0.00%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5jPXqZp-Do07"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}